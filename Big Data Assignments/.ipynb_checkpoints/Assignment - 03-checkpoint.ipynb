{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b68779",
   "metadata": {},
   "source": [
    "```\n",
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96231e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hadoop_config(config_file):\n",
    "    hadoop_config = {}\n",
    "    with open(config_file, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('dfs.namenode'):\n",
    "                hadoop_config['NameNode'] = line.split('=')[1].strip()\n",
    "            elif line.startswith('dfs.datanode'):\n",
    "                hadoop_config['DataNode'] = line.split('=')[1].strip()\n",
    "            elif line.startswith('mapreduce.framework.name'):\n",
    "                hadoop_config['MapReduce'] = line.split('=')[1].strip()\n",
    "            # Add more core components as needed\n",
    "    return hadoop_config\n",
    "\n",
    "# Example usage\n",
    "config_file = 'hadoop_config.txt'\n",
    "hadoop_components = read_hadoop_config(config_file)\n",
    "print(\"Hadoop Core Components:\")\n",
    "for component, address in hadoop_components.items():\n",
    "    print(component + \": \" + address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e51ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def calculate_directory_size(directory):\n",
    "    cmd = \"hadoop fs -du -s -h {}\".format(directory)\n",
    "    output = subprocess.check_output(cmd, shell=True).decode('utf-8')\n",
    "    size = output.split()[0]\n",
    "    return size\n",
    "\n",
    "# Example usage\n",
    "hdfs_directory = '/user/data'\n",
    "total_size = calculate_directory_size(hdfs_directory)\n",
    "print(\"Total file size in directory\", hdfs_directory, \"is\", total_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "def mapper(chunk):\n",
    "    words = chunk.split()\n",
    "    return Counter(words)\n",
    "\n",
    "def reducer(counters):\n",
    "    return sum(counters, Counter())\n",
    "\n",
    "def get_top_words(file_path, num_words):\n",
    "    with open(file_path, 'r') as file:\n",
    "        chunks = file.read().split()\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "    chunk_counts = pool.map(mapper, chunks)\n",
    "    word_counts = reducer(chunk_counts)\n",
    "    top_words = word_counts.most_common(num_words)\n",
    "    return top_words\n",
    "\n",
    "# Example usage\n",
    "text_file = 'large_text_file.txt'\n",
    "top_words_count = 10\n",
    "top_words = get_top_words(text_file, top_words_count)\n",
    "print(\"Top\", top_words_count, \"words:\")\n",
    "for word, count in top_words:\n",
    "    print(word + \": \" + str(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_namenode_status():\n",
    "    response = requests.get('http://namenode:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus')\n",
    "    data = response.json()\n",
    "    return data['beans'][0]['State']\n",
    "\n",
    "def check_datanode_status():\n",
    "    response = requests.get('http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo')\n",
    "    data = response.json()\n",
    "    return data['beans'][0]['State']\n",
    "\n",
    "# Example usage\n",
    "namenode_status = check_namenode_status()\n",
    "datanode_status = check_datanode_status()\n",
    "print(\"NameNode status:\", namenode_status)\n",
    "print(\"DataNode status:\", datanode_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(hdfs_path):\n",
    "    cmd = \"hadoop fs -ls {}\".format(hdfs_path)\n",
    "    output = subprocess.check_output(cmd, shell=True).decode('utf-8')\n",
    "    lines = output.split('\\n')[1:-1]\n",
    "    files_and_directories = [line.split()[-1] for line in lines]\n",
    "    return files_and_directories\n",
    "\n",
    "# Example usage\n",
    "hdfs_path = '/user/data'\n",
    "items = list_hdfs_path(hdfs_path)\n",
    "print(\"Items in\", hdfs_path + \":\")\n",
    "for item in items:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_datanode_storage():\n",
    "    response = requests.get('http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId')\n",
    "    data = response.json()\n",
    "    volumes = data['beans'][0]['VolumeInfo']\n",
    "    sorted_volumes = sorted(volumes, key=lambda x: x['usedSpace'], reverse=True)\n",
    "    highest_storage = sorted_volumes[0]\n",
    "    lowest_storage = sorted_volumes[-1]\n",
    "    return highest_storage, lowest_storage\n",
    "\n",
    "# Example usage\n",
    "highest_storage, lowest_storage = analyze_datanode_storage()\n",
    "print(\"DataNode with the highest storage capacity:\")\n",
    "print(\"Storage ID:\", highest_storage['storageID'])\n",
    "print(\"Used Space:\", highest_storage['usedSpace'])\n",
    "print(\"Remaining Space:\", highest_storage['remainingSpace'])\n",
    "print()\n",
    "print(\"DataNode with the lowest storage capacity:\")\n",
    "print(\"Storage ID:\", lowest_storage['storageID'])\n",
    "print(\"Used Space:\", lowest_storage['usedSpace'])\n",
    "print(\"Remaining Space:\", lowest_storage['remainingSpace'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8858b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(jar_file, input_path, output_path):\n",
    "    submit_url = 'http://resourcemanager:8088/ws/v1/cluster/apps/new-application'\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "    application_id = data['application-id']\n",
    "\n",
    "    submit_job_url = 'http://resourcemanager:8088/ws/v1/cluster/apps'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    payload = {\n",
    "        'application-id': application_id,\n",
    "        'application-name': 'My Hadoop Job',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar {} {} {}'.format(jar_file, input_path, output_path)\n",
    "            }\n",
    "        },\n",
    "        'application-type': 'MAPREDUCE'\n",
    "    }\n",
    "    response = requests.post(submit_job_url, headers=headers, json=payload)\n",
    "    return application_id\n",
    "\n",
    "def monitor_hadoop_job(application_id):\n",
    "    status_url = 'http://resourcemanager:8088/ws/v1/cluster/apps/{}'.format(application_id)\n",
    "    while True:\n",
    "        response = requests.get(status_url)\n",
    "        data = response.json()\n",
    "        state = data['app']['state']\n",
    "        if state == 'FINISHED':\n",
    "            return True\n",
    "        elif state in ['FAILED', 'KILLED']:\n",
    "            return False\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_hadoop_job_output(application_id):\n",
    "    output_url = 'http://resourcemanager:8088/ws/v1/cluster/apps/{}/appattempts'.format(application_id)\n",
    "    response = requests.get(output_url)\n",
    "    data = response.json()\n",
    "    final_output = data['appAttempts']['appAttempt'][0]['logsLink']\n",
    "    return final_output\n",
    "\n",
    "# Example usage\n",
    "jar_file = 'my_job.jar'\n",
    "input_path = '/user/input\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
